{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### CNN Training - Train delta\n",
    "original script from sustainlab_group: https://github.com/sustainlab-group/africa_poverty/blob/master/train_delta.py\n",
    "\n",
    "Tbd: Summary what happens in this notebook\n",
    "\n",
    "#### importing intern"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from models.resnet_model import Hyperspectral_Resnet\n",
    "from batchers import delta_batcher\n",
    "from utils.run import get_full_experiment_name, make_log_and_ckpt_dirs\n",
    "import utils.trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### importing extern"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### set root directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# edit if necessary\n",
    "ROOT_DIR = 'C:/Users/matte/Documents/Data/01_Universitaet/02_TH_Koeln/06_Semester/04_Machine_Learning_Project/CNN_Architecture'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `run_training`\n",
    "setting size of train and val sets, getting tfrecord_paths and in-country folds\n",
    "sub-function: `get_batcher` trains and evals model, saves results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def run_training(sess, ooc, batcher, dataset, keep_frac, model_name, model_params, batch_size,\n",
    "                 ls_bands, nl_band, label_name, orig_labels, weighted, augment, learning_rate,\n",
    "                 lr_decay, max_epochs, print_every, eval_every, num_threads, cache, log_dir,\n",
    "                 save_ckpt_dir, init_ckpt_dir, imagenet_weights_path, hs_weight_init,\n",
    "                 exclude_final_layer):\n",
    "    '''\n",
    "    Args\n",
    "    - sess: tf.Session\n",
    "    - ooc: bool, whether to use out-of-country split, must be False\n",
    "    - batcher: str, batcher type, one of ['delta', 'deltaclass']\n",
    "    - dataset: str, one of ['LSMSDeltaIncountry{f}', 'LSMSDeltaClassIncountry{f}', 'LSMSIndexOfDeltaIncountry{f}']\n",
    "    - keep_frac: float, only supports 1.0\n",
    "    - model_name: str, must be 'resnet'\n",
    "    - model_params: dict\n",
    "    - batch_size: int\n",
    "    - ls_bands: one of [None, 'rgb', 'ms']\n",
    "    - nl_band: one of [None, 'merge', 'split']\n",
    "    - label_name: str, name of the label in the TFRecord file\n",
    "    - orig_labels: bool, whether to include original labels for multi-task training\n",
    "    - weighted: bool, whether to weight clusters by household count in loss function\n",
    "    - augment: str\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float\n",
    "    - max_epochs: int\n",
    "    - print_every: int\n",
    "    - eval_every: int\n",
    "    - num_threads: int\n",
    "    - cache: list of str\n",
    "    - log_dir: str, path to directory to save logs for TensorBoard, must already exist\n",
    "    - save_ckpt_dir: str, path to checkpoint dir for saving weights\n",
    "        - intermediate dirs must already exist\n",
    "    - init_ckpt_dir: str, path to checkpoint dir from which to load existing weights\n",
    "        - set to empty string '' to use ImageNet or random initialization\n",
    "    - imagenet_weights_path: str, path to pre-trained weights from ImageNet\n",
    "        - set to empty string '' to use saved ckpt or random initialization\n",
    "    - hs_weight_init: str, one of [None, 'random', 'same', 'samescaled']\n",
    "    - exclude_final_layer: bool, or None\n",
    "    '''\n",
    "\n",
    "    # check if paths exist\n",
    "    assert os.path.exists(log_dir)\n",
    "    assert os.path.exists(os.path.dirname(save_ckpt_dir))\n",
    "\n",
    "    # check if keep_frac == 1.0\n",
    "    assert keep_frac == 1.0\n",
    "\n",
    "    # check model\n",
    "    if model_name != 'resnet':\n",
    "        raise NotImplementedError(f'Unsupported model_name: {model_name}')\n",
    "\n",
    "    # check ooc\n",
    "    if ooc:\n",
    "        raise NotImplementedError('OOC is currently not supported')\n",
    "\n",
    "    # further dataset and batcher error checking, set batcher and trainer\n",
    "    if 'LSMSDealtaIncountry' in dataset or 'LSMSIndexOfDeltaIncountry' in dataset:\n",
    "        assert batcher == 'delta'\n",
    "        Batcher = delta_batcher.DeltaBatcher\n",
    "        Trainer = utils.trainer.RegressionTrainer\n",
    "    elif 'LSMSDeltaClassIncountry' in dataset:\n",
    "        assert batcher == 'deltaclass'\n",
    "        assert not orig_labels\n",
    "        assert not weighted # sustainlab-group: currently not supported\n",
    "        Batcher = delta_batcher.DeltaClassBatcher\n",
    "        Trainer = utils.trainer.ClassificationTrainer\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported dataset:m {dataset}')\n",
    "\n",
    "    # set model class\n",
    "    model_class = Hyperspectral_Resnet\n",
    "\n",
    "    # load lsms incountry folds\n",
    "    with open(os.path.join(ROOT_DIR, 'data/lsms_incountry_folds.pkl'), 'rb') as f:\n",
    "        incountry_folds = pickle.load(f)\n",
    "    fold = dataset[-1] # sustainlab-group: last letter of dataset\n",
    "\n",
    "    # load delta pairs\n",
    "    delta_pairs_df = pd.read_csv(os.path.join(ROOT_DIR, 'data/lsmsdelta_pairs.csv'))\n",
    "\n",
    "    # define batchers for weighted\n",
    "    if weighted:\n",
    "        # sustainlab-group: we replace the label in the TFRecords with the label from the CSV\n",
    "        assert label_name is None\n",
    "        assert not orig_labels\n",
    "\n",
    "        train_extra_fields = {'labels': tf.placeholder(tf.float32, shape=[None]),\n",
    "                              'weights': tf.placeholder(tf.float32, shape=[None])}\n",
    "        val_extra_fields = {'labels': tf.placeholder(tf.float32, shape=[None]),\n",
    "                            'weights': tf.placeholder(tf.float32, shape=[None])}\n",
    "\n",
    "        # set label_col\n",
    "        if 'IndexOfDelta' in dataset:\n",
    "            label_col = 'index_diff'\n",
    "        else:\n",
    "            label_col = 'index'\n",
    "\n",
    "        # get tfrecord pairs\n",
    "        paths_dict, household_dict, labels_dict = delta_batcher.get_lsms_tfrecord_pairs(\n",
    "            indices_dict = incountry_folds[fold],\n",
    "            delta_pairs_df = delta_pairs_df,\n",
    "            index_cols = ['tfrecords_index.x', 'tfrecords_index.y'],\n",
    "            other_cols = ['x', label_col])\n",
    "\n",
    "        # get weights\n",
    "        weights_dict = {\n",
    "            split: households / np.sum(households) * len(households)\n",
    "            for split, households in households_dict.items()\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # sustainlab-group: use the labels in the TFRecords\n",
    "        train_extra_fields, val_extra_fields = None, None\n",
    "        paths_dict = delta_batcher.get_lsms_tfrecord_pairs(\n",
    "            indices_dict = incountry_folds[fold],\n",
    "            delta_pairs_df = delta_pairs_df,\n",
    "            index_cols = ['tfrecords_index.x', 'tfrecords_index.y']\n",
    "        )\n",
    "\n",
    "    # calculate dataset size and steps per epoch\n",
    "    num_train = len(paths_dict['train'])\n",
    "    num_val = len(paths_dict['val'])\n",
    "    print('Train pairs:', num_train)\n",
    "    print('Val pairs:', num_val)\n",
    "    train_steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "    val_steps_per_epoch = int(np.ceil(num_val / batch_size))\n",
    "\n",
    "    def get_batcher(tfrecord_paths, shuffle, augment, epochs, cache, orig_labels, extra_fields=None):\n",
    "        kwargs = dict(\n",
    "            tfrecord_pairs = tfrecord_pairs,\n",
    "            dataset = 'LSMS',\n",
    "            batch_size = batch_size,\n",
    "            label_name = label_name,\n",
    "            num_threads = num_threads,\n",
    "            epochs = epochs,\n",
    "            ls_bands = ls_bands,\n",
    "            nl_band = nl_band,\n",
    "            shuffle = shuffle,\n",
    "            augment = augment,\n",
    "            normalize = True,\n",
    "            cache = cache\n",
    "        )\n",
    "\n",
    "        # set orig_labels and extra_fields\n",
    "        if batcher == 'delta':\n",
    "            kwargs['orig_labels'] = orig_labels\n",
    "        if extra_fields is not None:\n",
    "            kwargs['extra_field'] = extra_fields\n",
    "\n",
    "        return batcher(**kwargs)\n",
    "\n",
    "    # set placeholders for tensors\n",
    "    train_tfrecord_pairs_ph = tf.compat.v1.placeholder(tf.string, shape=[None, 2])\n",
    "    val_tfrecord_pairs_ph = tf.compat.v1.placeholder(tf.string, shape=[None, 2])\n",
    "\n",
    "    # get train bdelta_batcher\n",
    "    with tf.name_scope('train_batcher'):\n",
    "        train_batcher = get_batcher(\n",
    "            shuffle = True,\n",
    "            augment = augment,\n",
    "            epochs = max_epochs,\n",
    "            cache = 'train' in cache,\n",
    "            orig_labels = orig_labels,\n",
    "            extra_fields = train_extra_fields\n",
    "        )\n",
    "        train_init_iter, train_batch = train_batcher.get_batch()\n",
    "\n",
    "    # get train, eval batch\n",
    "    with tf.name_scope('train_eval_batcher'):\n",
    "        train_eval_batcher = get_batcher(\n",
    "            train_tfrecord_pairs_ph,\n",
    "            shuffle = False,\n",
    "            augment = 'none',\n",
    "            epochs = max_epochs +1, # sustainlab: may need extra epoch at the end of training\n",
    "            cache = 'train_eval' in cache,\n",
    "            orig_labels = orig_labels,\n",
    "            extra_fields = train_extra_fields\n",
    "        )\n",
    "        train_eval_init_iter, train_eval_batch = train_eval_batcher.get_batch()\n",
    "\n",
    "    # get val batch\n",
    "    with tf.name_scope('val_batcher'):\n",
    "        val_batcher = get_batcher(\n",
    "            val_tfrecord_paths_ph,\n",
    "            shuffle = False,\n",
    "            augment = 'none',\n",
    "            epochs = max_epochs +1, # sustainlab: may need extra epoch at the end of training\n",
    "            cache = 'val' in cache,\n",
    "            orig_labels = orig_labels,\n",
    "            extra_fields = val_extra_fields\n",
    "        )\n",
    "        val_init_iter, val_batch = val_batcher.get_batch()\n",
    "\n",
    "    # build model\n",
    "    print('Building model...')\n",
    "    if orig_labels or batcher == 'deltaclass':\n",
    "        model_params['num_outputs'] = 3\n",
    "    else:\n",
    "        model_params['num_outputs'] = 1\n",
    "\n",
    "    # train model\n",
    "    with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope()) as model_scope: # edited mm\n",
    "        train_model = model_class(train_batch['images'], is_training = True, **model_params)\n",
    "        train_preds = tf.squeeze(train_model.outputs, name = 'train_preds')\n",
    "\n",
    "    # train, eval model\n",
    "    with tf.compat.v1.variable_scope(model_scope, reuse = True): # edited mm\n",
    "        train_eval_model = model_class(train_eval_batch['images'], is_training = False, **model_params)\n",
    "        train_eval_preds = tf.squeeze(train_eval_model.outputs, name = 'train_eval_preds')\n",
    "\n",
    "    # val model\n",
    "    with tf.compat.v1.variable_scope(model_scope, reuse = True): # edited mm\n",
    "        val_model = model_class(val_batch['images'], is_training = False, **model_params)\n",
    "        val_preds = tf.squeeze(val_model.outputs, name = 'val_preds')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        train_batch, train_eval_batch, val_batch,\n",
    "        train_model, train_eval_model, val_model,\n",
    "        train_preds, train_eval_preds, val_preds,\n",
    "        sess, train_steps_per_epoch, ls_bands, nl_band, learning_rate, lr_decay,\n",
    "        log_dir, save_ckpt_dir, init_ckpt_dir, imagenet_weights_path,\n",
    "        hs_weight_init, exclude_final_layer, image_summaries=False)\n",
    "\n",
    "    # sustainlab-group: initialize the training dataset iterator\n",
    "    feed_dict = {\n",
    "        train_tfrecord_pairs_ph: paths_dict['train'],\n",
    "        val_tfrecord_pairs_ph: paths_dict['val']\n",
    "    }\n",
    "\n",
    "    # update feed_dict if weighted, add train extra fields and val extra fields\n",
    "    if weighted:\n",
    "        feed_dict.update({\n",
    "            train_extra_fields['labels']: labels_dict['train'],\n",
    "            train_extra_fields['weights']: weights_dict['train'],\n",
    "            val_extra_fields['labels']: labels_dict['val'],\n",
    "            val_extra_fields['weights']: weights_dict['val'],\n",
    "        })\n",
    "\n",
    "    # run session\n",
    "    sess.run([train_init_iter, train_eval_init_iter, val_init_iter], feed_dict=feed_dict)\n",
    "\n",
    "    # train epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        if epoch % eval_every == 0:\n",
    "            trainer.eval_train(max_nbatches=train_steps_per_epoch)\n",
    "            trainer.eval_val(max_nbatches=val_steps_per_epoch)\n",
    "        trainer.train_epoch(print_every)\n",
    "\n",
    "    # eval train\n",
    "    trainer.eval_train(max_nbatches=train_steps_per_epoch)\n",
    "    trainer.eval_val(max_nbatches=val_steps_per_epoch)\n",
    "\n",
    "    # save log results\n",
    "    csv_log_path = os.path.join(log_dir, 'results.csv')\n",
    "    trainer.log_results(csv_log_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `run_training_wrapper`\n",
    "hand over params, create and config session, print elapsed time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def run_training_wrapper(**params):\n",
    "    '''\n",
    "    params is a dict with keys matching the FLAGS defined below\n",
    "    '''\n",
    "\n",
    "    # print starting time\n",
    "    start = time.time()\n",
    "    print('Current time:', start)\n",
    "\n",
    "    # print all of the flags\n",
    "    pprint(params)\n",
    "\n",
    "    # parameters that might be 'None'\n",
    "    none_params = ['label_name', 'ls_bands', 'nl_band', 'exclude_final_layer',\n",
    "                   'hs_weight_init', 'imagenet_weights_path', 'init_ckpt_dir']\n",
    "    for p in none_params:\n",
    "        if params[p] == 'None':\n",
    "            params[p] = None\n",
    "\n",
    "    # reset any existing graph\n",
    "    tf.compat.v1.reset_default_graph() # edited mm\n",
    "\n",
    "    # set the random seeds\n",
    "    seed = params['seed']\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed) # edited mm\n",
    "\n",
    "    # create the log and checkpoint directories if needed\n",
    "    full_experiment_name = get_full_experiment_name(\n",
    "        params['experiment_name'], params['batch_size'],\n",
    "        params['fc_reg'], params['conv_reg'], params['lr'])\n",
    "    log_dir, ckpt_prefix = make_log_and_ckpt_dirs(\n",
    "        params['log_dir'], params['ckpt_dir'], full_experiment_name)\n",
    "    print(f'Checkpoint prefix: {ckpt_prefix}')\n",
    "\n",
    "    # check for previous run\n",
    "    params_filepath = os.path.join(log_dir, 'params.txt')\n",
    "    assert not os.path.exists(params_filepath), f'Stopping. Found previous run at: {params_filepath}'\n",
    "    with open(params_filepath, 'w') as f:\n",
    "        pprint(params, stream=f)\n",
    "        pprint(f'Checkpoint prefix: {ckpt_prefix}', stream=f)\n",
    "\n",
    "    # create session\n",
    "    # sustainlab: - MUST set os.environ['CUDA_VISIBLE_DEVICES'] before creating tf.Session object\n",
    "    if params['gpu'] is None: # sustainlab: restrict to CPU only\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(params['gpu'])\n",
    "\n",
    "    # configure session\n",
    "    config = tf.compat.v1.ConfigProto() # edited mm\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config = config) # edited mm\n",
    "\n",
    "    # set model parameters\n",
    "    model_params = {\n",
    "        'fc_reg': params['fc_reg'],\n",
    "        'conv_reg': params['conv_reg'],\n",
    "        'use_dilated_conv_in_first_layer': False,\n",
    "    }\n",
    "\n",
    "    if params['model_name'] == 'resnet':\n",
    "        model_params['num_layers'] = params['num_layers']\n",
    "\n",
    "    run_training(\n",
    "        sess = sess,\n",
    "        ooc = params['ooc'],\n",
    "        batcher = params['batcher'],\n",
    "        dataset = params['dataset'],\n",
    "        keep_frac = params['keep_frac'],\n",
    "        model_name = params['model_name'],\n",
    "        model_params = model_params,\n",
    "        batch_size = params['batch_size'],\n",
    "        ls_bands = params['ls_bands'],\n",
    "        nl_band = params['nl_band'],\n",
    "        label_name = params['label_name'],\n",
    "        orig_labels = params['orig_labels'],\n",
    "        weighted = params['weighted'],\n",
    "        augment = params['augment'],\n",
    "        learning_rate = params['lr'],\n",
    "        lr_decay = params['lr_decay'],\n",
    "        max_epochs = params['max_epochs'],\n",
    "        print_every = params['print_every'],\n",
    "        eval_every = params['eval_every'],\n",
    "        num_threads = params['num_threads'],\n",
    "        cache = params['cache'],\n",
    "        log_dir = log_dir,\n",
    "        save_ckpt_dir = ckpt_prefix,\n",
    "        init_ckpt_dir = params['init_ckpt_dir'],\n",
    "        imagenet_weights_path = params['imagenet_weights_path'],\n",
    "        hs_weight_init = params['hs_weight_init'],\n",
    "        exclude_final_layer = params['exclude_final_layer']\n",
    "    )\n",
    "    sess.close()\n",
    "\n",
    "    # print ending time\n",
    "    end = time.time()\n",
    "    print('End time:', end)\n",
    "    print('Time elapsed (sec.):', end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `main`\n",
    "hand over flags to params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    params = {\n",
    "        key: flags.FLAGS.__getattr__(key)\n",
    "        for key in dir(flags.FLAGS)\n",
    "    }\n",
    "    run_training_wrapper(**params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "define flags and start process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'verbosity'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-19-c317d620e103>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mflags\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDEFINE_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'seed'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m123\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'seed for random initialization and shuffling'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# edited mm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(main, argv)\u001B[0m\n\u001B[0;32m     38\u001B[0m   \u001B[0mmain\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmain\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0m_sys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'__main__'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m   \u001B[0m_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags_parser\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_parse_flags_tolerate_undef\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\app.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(main, argv, flags_parser)\u001B[0m\n\u001B[0;32m    295\u001B[0m     args = _run_init(\n\u001B[0;32m    296\u001B[0m         \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margv\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0margv\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 297\u001B[1;33m         \u001B[0mflags_parser\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    298\u001B[0m     )\n\u001B[0;32m    299\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[0m_init_callbacks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\app.py\u001B[0m in \u001B[0;36m_run_init\u001B[1;34m(argv, flags_parser)\u001B[0m\n\u001B[0;32m    361\u001B[0m   \u001B[0mcommand_name\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_process_name_useful\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m   \u001B[1;31m# Set up absl logging handler.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 363\u001B[1;33m   \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse_absl_handler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m   args = _register_and_parse_flags_with_usage(\n\u001B[0;32m    365\u001B[0m       \u001B[0margv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\logging\\__init__.py\u001B[0m in \u001B[0;36muse_absl_handler\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1227\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mabsl_handler\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandlers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1228\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mabsl_handler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1229\u001B[1;33m     \u001B[0mFLAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'verbosity'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_logging_levels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1230\u001B[0m     \u001B[0mFLAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'logger_levels'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_logger_levels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1231\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    468\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    469\u001B[0m     \u001B[1;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 470\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flags\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    471\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    472\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_hide_flag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'verbosity'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # delete all flags, edit mm\n",
    "    def del_all_flags(FLAGS):\n",
    "        flags_dict = FLAGS._flags()\n",
    "        keys_list = [keys for keys in flags_dict]\n",
    "        for keys in keys_list:\n",
    "            FLAGS.__delattr__(keys)\n",
    "    del_all_flags(tf.compat.v1.flags.FLAGS)\n",
    "\n",
    "    flags = tf.compat.v1.app.flags # edited mm\n",
    "\n",
    "    # paths\n",
    "    flags.DEFINE_string('experiment_name', 'new_experiment', 'name of the experiment being run')\n",
    "    flags.DEFINE_string('ckpt_dir', os.path.join(ROOT_DIR, 'ckpts/'), 'checkpoint directory')\n",
    "    flags.DEFINE_string('log_dir', os.path.join(ROOT_DIR, 'logs/'), 'log directory')\n",
    "\n",
    "    # initialization\n",
    "    flags.DEFINE_string('init_ckpt_dir', None, 'path to checkpoint prefix from which to initialize weights (default None)')\n",
    "    flags.DEFINE_string('imagenet_weights_path', None, 'path to ImageNet weights for initialization (default None)')\n",
    "    flags.DEFINE_string('hs_weight_init', None, 'method for initializing weights of non-RGB bands in 1st conv layer, one of [None (default), \"random\", \"same\", \"samescaled\"]')\n",
    "    flags.DEFINE_boolean('exclude_final_layer', None, 'whether to use checkpoint to initialize final layer (default None)')\n",
    "\n",
    "    # learning parameters\n",
    "    flags.DEFINE_string('label_name', 'wealthpooled', 'name of label to use from the TFRecord files')\n",
    "    flags.DEFINE_boolean('orig_labels', False, 'whether to include original labels for multi-task training')\n",
    "    flags.DEFINE_boolean('weighted', True, 'whether to weight clusters by household count in loss function')\n",
    "    flags.DEFINE_integer('batch_size', 64, 'batch size')\n",
    "    flags.DEFINE_string('augment', 'bidir', 'whether to use data augmentation, one of [\"none\", \"bidir\", \"forward\"]')\n",
    "    flags.DEFINE_float('fc_reg', 1e-3, 'regularization penalty factor for fully connected layers')\n",
    "    flags.DEFINE_float('conv_reg', 1e-3, 'regularization penalty factor for convolution layers')\n",
    "    flags.DEFINE_float('lr', 1e-3, 'learning rate')\n",
    "    flags.DEFINE_float('lr_decay', 1.0, 'decay rate of the learning rate (default 1.0 for no decay)')\n",
    "\n",
    "    # high-level model control\n",
    "    flags.DEFINE_string('model_name', 'resnet', 'name of the model to be used, currently only \"resnet\" is supported')\n",
    "\n",
    "    # resnet-only params\n",
    "    flags.DEFINE_integer('num_layers', 18, 'number of ResNet layers, one of [18 (default), 34, 50]')\n",
    "\n",
    "    # data params\n",
    "    flags.DEFINE_string('batcher', 'delta', 'batcher type, one of [\"delta\", \"deltaclass\"]')\n",
    "    flags.DEFINE_string('dataset', 'LSMSDeltaIncountryA', 'dataset to use, options depend on batcher (default \"LSMSDeltaIncountryA\")')\n",
    "    flags.DEFINE_boolean('ooc', False, 'whether to use out-of-country split (default False)')\n",
    "    flags.DEFINE_float('keep_frac', 1.0, 'fraction of training data to use (default 1.0)')\n",
    "    flags.DEFINE_string('ls_bands', None, 'Landsat bands to use, one of [None (default), \"rgb\", \"ms\"]')\n",
    "    flags.DEFINE_string('nl_band', None, 'nightlights band, one of [None (default), \"merge\", \"split\"]')\n",
    "\n",
    "    # system\n",
    "    flags.DEFINE_integer('gpu', 0, 'which GPU to use (default 0)')\n",
    "    flags.DEFINE_integer('num_threads', 1, 'number of threads for batcher (default 1)')\n",
    "    flags.DEFINE_list('cache', [], 'comma-separated list (no spaces) of datasets to cache in memory, choose from [None, \"train\", \"train_eval\", \"val\"]')\n",
    "\n",
    "    # Misc\n",
    "    flags.DEFINE_integer('max_epochs', 200, 'maximum number of epochs for training (default 200)')\n",
    "    flags.DEFINE_integer('eval_every', 1, 'evaluate model on validation set after every so many epochs of training')\n",
    "    flags.DEFINE_integer('print_every', 10, 'print training statistics after every so many steps')\n",
    "    flags.DEFINE_integer('seed', 123, 'seed for random initialization and shuffling')\n",
    "\n",
    "    tf.compat.v1.app.run() # edited mm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}