{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### CNN Training - Train directly\n",
    "Tbd: Summary what happens in this notebook\n",
    "\n",
    "#### importing intern"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from models.resnet_model import Hyperspectral_Resnet\n",
    "from batchers import dataset_constants, batcher\n",
    "from utils.run import get_full_experiment_name, make_log_and_ckpt_dirs\n",
    "from utils.trainer import RegressionTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### importing extern"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### set root directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# edit if necessary\n",
    "# ROOT_DIR = 'C:/Users/matte/Documents/Data/01_Universitaet/02_TH_Koeln/06_Semester/\n",
    "# 04_Machine_Learning_Project/CNN_Architecture'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `run_training`\n",
    "setting size of train and val sets, getting tfrecord_paths and in-country folds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def run_training(sess, ooc, batcher_type, dataset, keep_frac, model_name, model_params, batch_size,\n",
    "                 ls_bands, nl_band, label_name, augment, learning_rate, lr_decay,\n",
    "                 max_epochs, print_every, eval_every, num_threads, cache, log_dir, save_ckpt_dir,\n",
    "                 init_ckpt_dir, imagenet_weights_path, hs_weight_init, exclude_final_layer):\n",
    "    '''\n",
    "    Args\n",
    "    - sess: tf.Session\n",
    "    - ooc: bool, whether to use out-of-country split\n",
    "    - batcher_type: str, type of batcher, one of ['base', 'urban', 'rural']\n",
    "    - dataset: str, options depends on batcher_type\n",
    "    - keep_frac: float\n",
    "    - model_name: str, one of ['resnet', 'vggf', 'simplecnn', 'resnetcombo']\n",
    "    - model_params: dict\n",
    "    - batch_size: int\n",
    "    - ls_bands: one of [None, 'rgb', 'ms']\n",
    "    - nl_band: one of [None, 'merge', 'split']\n",
    "    - label_name: str, name of the label in the TFRecord file\n",
    "    - augment: bool\n",
    "    - learning_rate: float\n",
    "    - lr_decay: float\n",
    "    - max_epochs: int\n",
    "    - print_every: int\n",
    "    - eval_every: int\n",
    "    - num_threads: int\n",
    "    - cache: list of str\n",
    "    - log_dir: str, path to directory to save logs for TensorBoard, must already exist\n",
    "    - save_ckpt_dir: str, path to checkpoint dir for saving weights\n",
    "        - intermediate dirs must already exist\n",
    "    - init_ckpt_dir: str, path to checkpoint dir from which to load existing weights\n",
    "        - set to empty string '' to use ImageNet or random initialization\n",
    "    - imagenet_weights_path: str, path to pre-trained weights from ImageNet\n",
    "        - set to empty string '' to use saved ckpt or random initialization\n",
    "    - hs_weight_init: str, one of [None, 'random', 'same', 'samescaled']\n",
    "    - exclude_final_layer: bool, or None\n",
    "    '''\n",
    "\n",
    "    # set model class\n",
    "    if model_name == 'resnet':\n",
    "        model_class = Hyperspectral_Resnet\n",
    "\n",
    "    # import model classes if needed\n",
    "    elif model_name == 'vggf':\n",
    "        model_class = VGGF\n",
    "    elif model_name == 'simplecnn':\n",
    "        model_class = SimpleCNN\n",
    "    elif model_name == 'resnetcombo':\n",
    "        model_class = ResnetCombo\n",
    "    else:\n",
    "        raise ValueError('Unknown model_name. Was not one of [\"resnet\", \"vggf\", \"simplecnn\", \"resnetcombo\"]'\n",
    "                         'or model was not imported.')\n",
    "    # check if paths exist\n",
    "    assert os.path.exists(log_dir)\n",
    "    assert os.path.exists(os.path.dirname(save_ckpt_dir))\n",
    "\n",
    "    # batcher definition\n",
    "    # out-of-country split for dhs\n",
    "    if ooc:\n",
    "\n",
    "        # sustainlab: temporary hack: hard-coding '2009-17' base dataset for all DHS OOC\n",
    "        base_dataset = '2009-17'\n",
    "\n",
    "        # get tfrecord paths for train, val and all data\n",
    "        train_tfrecord_paths = np.asarray(batcher.get_tfrecord_paths(dataset, 'train'))\n",
    "        val_tfrecord_paths = np.asarray(batcher.get_tfrecord_paths(dataset, 'val'))\n",
    "        all_tfrecord_paths = np.asarray(batcher.get_tfrecord_paths(dataset, 'all'))\n",
    "\n",
    "        # get dataset sizes\n",
    "        sizes = {\n",
    "            'base': dataset_constants.SIZES[dataset],\n",
    "            'urban': dataset_constants.URBAN_SIZES[dataset],\n",
    "            'rural': dataset_constants.RURAL_SIZES[dataset]\n",
    "        }[batcher_type]\n",
    "\n",
    "        # check size of tfrecord paths\n",
    "        assert len(train_tfrecord_paths) == sizes['train']\n",
    "        assert len(val_tfrecord_paths) == sizes['val']\n",
    "\n",
    "    # in-country split\n",
    "    else:\n",
    "        if batcher_type != 'base':\n",
    "            raise ValueError('incountry w/ non-base batcher is not supported')\n",
    "\n",
    "        # get tfrecord paths and in-country folds for lsms\n",
    "        if ('lsms' in dataset.lower()) and ('incountry' in dataset.lower()):\n",
    "            base_dataset = 'LSMS'\n",
    "            all_cys = dataset_constants.SURVEY_NAMES['LSMS']\n",
    "            all_tfrecord_paths = np.asarray(batcher.get_tfrecord_paths(all_cys))\n",
    "\n",
    "            # get incountry folds\n",
    "            with open(os.path.join(ROOT_DIR, 'data/lsms_incountry_folds.pkl'), 'rb') as f:\n",
    "                incountry_folds = pickle.load(f)\n",
    "\n",
    "            # check size of incountry folds\n",
    "            assert len(all_tfrecord_paths) == dataset_constants.SIZES['LSMSincountry']['all']\n",
    "\n",
    "        # get tfrecord paths and in-country folds for dhs\n",
    "        else:\n",
    "\n",
    "            # sustainlab: hard-coding '2009-17' dataset for all DHS in-country\n",
    "            base_dataset = '2009-17'\n",
    "            all_tfrecord_paths = np.asarray(batcher.get_tfrecord_paths('2009-17', 'all'))\n",
    "\n",
    "            # get incountry folds\n",
    "            with open(os.path.join(ROOT_DIR, 'data/dhs_incountry_folds.pkl'), 'rb') as f:\n",
    "                incountry_folds = pickle.load(f)\n",
    "\n",
    "            # check size of incountry folds\n",
    "            assert len(all_tfrecord_paths) == dataset_constants.SIZES[dataset]['all']\n",
    "\n",
    "        # define indices, tfrecord paths for train and val data\n",
    "        fold = dataset[-1]\n",
    "        train_indices = incountry_folds[fold]['train']\n",
    "        val_indices = incountry_folds[fold]['val']\n",
    "\n",
    "        train_tfrecord_paths = all_tfrecord_paths[train_indices]\n",
    "        val_tfrecord_paths = all_tfrecord_paths[val_indices]\n",
    "\n",
    "    # get size of train and val data\n",
    "    num_train = len(train_tfrecord_paths)\n",
    "    num_val = len(val_tfrecord_paths)\n",
    "\n",
    "    # keep_frac affects sizes of both training and validation sets\n",
    "    if keep_frac < 1.0:\n",
    "        if batcher_type != 'base':\n",
    "            raise ValueError('keep_frac < 1.0 w/ non-base batcher is not supported')\n",
    "\n",
    "        # apply keep_frac on set sizes\n",
    "        num_train = int(num_train * keep_frac)\n",
    "        num_val = int(num_val * keep_frac)\n",
    "\n",
    "        # get randomized tfrecord paths\n",
    "        train_tfrecord_paths = np.random.choice(train_tfrecord_paths, size=num_train, replace=False)\n",
    "        val_tfrecord_paths = np.random.choice(val_tfrecord_paths, size=num_val, replace=False)\n",
    "\n",
    "    print('num_train:', num_train)\n",
    "    print('num_val:', num_val)\n",
    "\n",
    "    # calculate steps per epoch for train and val\n",
    "    train_steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "    val_steps_per_epoch = int(np.ceil(num_val / batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `get_batcher`\n",
    "train and eval model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def get_batcher(tfrecord_paths, shuffle, augment, epochs, cache):\n",
    "\n",
    "    # define BatcherClass\n",
    "    BatcherClass = {\n",
    "        'base': batcher.Batcher,\n",
    "        'urban': batcher.UrbanBatcher,\n",
    "        'rural': batcher.RuralBatcher,\n",
    "    }[batcher_type]\n",
    "\n",
    "    return BatcherClass(\n",
    "        tfrecord_files = tfrecord_paths,\n",
    "        dataset = base_dataset,\n",
    "        batch_size = batch_size,\n",
    "        label_name = label_name,\n",
    "        num_threads = num_threads,\n",
    "        epochs = epochs,\n",
    "        ls_bands = ls_bands,\n",
    "        nl_band = nl_band,\n",
    "        shuffle = shuffle,\n",
    "        augment = augment,\n",
    "        negatives = 'zero',\n",
    "        normalize = True,\n",
    "        cache = cache\n",
    "    )\n",
    "\n",
    "    # set placeholders for tensors\n",
    "    train_tfrecord_paths_ph = tf.compat.v1.placeholder(tf.string, shape=[None])\n",
    "    val_tfrecord_paths_ph = tf.compat.v1.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    # get train batch\n",
    "    with tf.name_scope('train_batcher'):\n",
    "        train_batcher = get_batcher(\n",
    "            train_tfrecord_paths_ph,\n",
    "            shuffle = True,\n",
    "            augment = augment,\n",
    "            epochs = max_epochs,\n",
    "            cache = 'train' in cache\n",
    "        )\n",
    "        train_init_iter, train_batch = train_batcher.get_batch()\n",
    "\n",
    "    # get train, eval batch\n",
    "    with tf.name_scope('train_eval_batcher'):\n",
    "        train_eval_batcher = get_batcher(\n",
    "            train_tfrecord_paths_ph,\n",
    "            shuffle = False,\n",
    "            augment = False,\n",
    "            epochs = max_epochs +1, # sustainlab: may need extra epoch at the end of training\n",
    "            cache = 'train_eval' in cache\n",
    "        )\n",
    "        train_eval_init_iter, train_eval_batch = train_eval_batcher.get_batch()\n",
    "\n",
    "    # get val batch\n",
    "    with tf.name_scope('val_batcher'):\n",
    "        val_batcher = get_batcher(\n",
    "            val_tfrecord_paths_ph,\n",
    "            shuffle = False,\n",
    "            augment = False,\n",
    "            epochs = max_epochs +1, # sustainlab: may need extra epoch at the end of training\n",
    "            cache = 'val' in cache\n",
    "        )\n",
    "        val_init_iter, val_batch = val_batcher.get_batch()\n",
    "\n",
    "    # build model\n",
    "    print('Building mode...', flush = True)\n",
    "    model_params['num_outpus'] = 1\n",
    "\n",
    "    # train model\n",
    "    with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope()) as model_scope: # edited mm\n",
    "        train_model = model_class(train_batch['images'], is_training = True, **model_params)\n",
    "        train_preds = train_model.outputs\n",
    "        if model_params['num_outputs'] == 1:\n",
    "            train_preds = tf.reshape(train_preds, shape = [-1], name = 'train_preds')\n",
    "\n",
    "    # train, eval model\n",
    "    with tf.compat.v1.variable_scope(model_scope, reuse = True): # edited mm\n",
    "        train_eval_model = model_class(train_eval_batch['images'], is_training = False, **model_params)\n",
    "        train_eval_preds = train_eval_model.outputs\n",
    "        if model_params['num_outputs'] == 1:\n",
    "            train_eval_preds = tf.reshape(train_eval_preds, shape = [-1], name = 'train_eval_preds')\n",
    "\n",
    "    # val model\n",
    "    with tf.compat.v1.variable_scope(model_scope, reuse = True): # edited mm\n",
    "        val_model = model_class(val_batch['images'], is_training = False, **model_params)\n",
    "        val_preds = val_model.outputs\n",
    "        if model_params['num_outputs'] == 1:\n",
    "            val_preds = tf.reshape(val_preds, shape = [-1], name = 'val_preds')\n",
    "\n",
    "    trainer = RegressionTrainer(\n",
    "        train_batch, train_eval_batch, val_batch,\n",
    "        train_model, train_eval_model, val_model,\n",
    "        train_preds, train_eval_preds, val_preds,\n",
    "        sess, train_steps_per_epoch, ls_bands, nl_band, learning_rate, lr_decay,\n",
    "        log_dir, save_ckpt_dir, init_ckpt_dir, imagenet_weights_path,\n",
    "        hs_weight_init, exclude_final_layer, image_summaries = False)\n",
    "\n",
    "    # sustainlab: initialize the training dataset iterator\n",
    "    sess.run([train_init_iter, train_eval_init_iter, val_init_iter], feed_dict={\n",
    "        train_tfrecord_paths_ph: train_tfrecord_paths,\n",
    "        val_tfrecord_paths_ph: val_tfrecord_paths\n",
    "    })\n",
    "\n",
    "    # train epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        if epoch % eval_every == 0:\n",
    "            trainer.eval_train(max_nbatches = train_steps_per_epoch)\n",
    "            trainer.eval_val(max_nbatches = val_steps_per_epoch)\n",
    "        trainer.train_epoch(print_every)\n",
    "\n",
    "    # eval train\n",
    "    trainer.eval_train(max_nbatches = train_steps_per_epoch)\n",
    "    trainer.eval_val(max_nbatches = val_steps_per_epoch)\n",
    "\n",
    "    # save log results\n",
    "    csv_log_path = os.path.join(log_dir, 'results.csv')\n",
    "    trainer.log_results(csv_log_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `run_training_wrapper`\n",
    "hand over params, create and config session, print elapsed time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def run_training_wrapper(**params):\n",
    "    '''\n",
    "    params is a dict with keys matching the FLAGS defined below\n",
    "    '''\n",
    "\n",
    "    # print starting time\n",
    "    start = time.time()\n",
    "    print('Current time:', start)\n",
    "\n",
    "    # print all of the flags\n",
    "    pprint(params)\n",
    "\n",
    "    # parameters that might be 'None'\n",
    "    none_params = ['ls_bands', 'nl_band', 'exclude_final_layer', 'hs_weight_init',\n",
    "                   'imagenet_weights_path', 'init_ckpt_dir']\n",
    "    for p in none_params:\n",
    "        if params[p] == 'None':\n",
    "            params[p] = None\n",
    "\n",
    "    # reset any existing graph\n",
    "    tf.compat.v1.reset_default_graph() # edited mm\n",
    "\n",
    "    # set the random seeds\n",
    "    seed = params['seed']\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed) # edited mm\n",
    "\n",
    "    # create the log and checkpoint directories if needed\n",
    "    full_experiment_name = get_full_experiment_name(\n",
    "        params['experiment_name'], params['batch_size'],\n",
    "        params['fc_reg'], params['conv_reg'], params['lr']\n",
    "    )\n",
    "    log_dir, ckpt_prefix = make_log_and_ckpt_dirs(\n",
    "        params['log_dir'], params['ckpt_dir'], full_experiment_name\n",
    "    )\n",
    "    print(f'Checkpoint prefix: {ckpt_prefix}')\n",
    "    params_filepath = os.path.join(log_dir, 'params.txt')\n",
    "\n",
    "    # check for previous run\n",
    "    assert not os.path.exists(params_filepath), f'Stopping. Found previous run at: {params_filepath}'\n",
    "    with open(params_filepath, 'w') as f:\n",
    "        pprint(params, stream = f)\n",
    "        pprint(f'Checkpoint prefix: {ckpt_prefix}', stream = f)\n",
    "\n",
    "    # create session\n",
    "    # sustainlab: - MUST set os.environ['CUDA_VISIBLE_DEVICES'] before creating tf.Session object\n",
    "    if params['gpu'] is None: # sustainlab: restrict to CPU only\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(params['gpu'])\n",
    "\n",
    "    # configure session\n",
    "    config = tf.compat.v1.ConfigProto() # edited mm\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config = config) # edited mm\n",
    "\n",
    "    # set model parameters\n",
    "    model_params = {\n",
    "        'fc_reg': params['fc_reg'],\n",
    "        'conv_reg': params['conv_reg'],\n",
    "        'use_dilated_conv_in_first_layer': False,\n",
    "    }\n",
    "\n",
    "    if params['model_name'] == 'resnet':\n",
    "        model_params['num_layers'] = params['num_layers']\n",
    "\n",
    "    run_training(\n",
    "        sess = sess,\n",
    "        ooc = params['ooc'],\n",
    "        batcher_type = params['batcher_type'],\n",
    "        dataset = params['dataset'],\n",
    "        keep_frac = params['keep_frac'],\n",
    "        model_name = params['model_name'],\n",
    "        model_params = model_params,\n",
    "        batch_size = params['batch_size'],\n",
    "        ls_bands = params['ls_bands'],\n",
    "        nl_band = params['nl_band'],\n",
    "        label_name = params['label_name'],\n",
    "        augment = params['augment'],\n",
    "        learning_rate = params['lr'],\n",
    "        lr_decay = params['lr_decay'],\n",
    "        max_epochs = params['max_epochs'],\n",
    "        print_every = params['print_every'],\n",
    "        eval_every = params['eval_every'],\n",
    "        num_threads = params['num_threads'],\n",
    "        cache = params['cache'],\n",
    "        log_dir = log_dir,\n",
    "        save_ckpt_dir = ckpt_prefix,\n",
    "        init_ckpt_dir = params['init_ckpt_dir'],\n",
    "        imagenet_weights_path = params['imagenet_weights_path'],\n",
    "        hs_weight_init = params['hs_weight_init'],\n",
    "        exclude_final_layer = params['exclude_final_layer']\n",
    "    )\n",
    "    sess.close()\n",
    "\n",
    "    # print ending time\n",
    "    end = time.time()\n",
    "    print('End time:', end)\n",
    "    print('Time elapsed (sec.):', end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### define `main`\n",
    "hand over flags to params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    params = {\n",
    "        key: flags.FLAGS.__getattr__(key)\n",
    "        for key in dir(flags.FLAGS)\n",
    "    }\n",
    "    run_training_wrapper(**params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "define flags and start process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'verbosity'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-52-94e2074fe482>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[0mflags\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDEFINE_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'seed'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m123\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'seed for random initialization and shuffling'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# edited mm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(main, argv)\u001B[0m\n\u001B[0;32m     38\u001B[0m   \u001B[0mmain\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmain\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0m_sys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'__main__'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m   \u001B[0m_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags_parser\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_parse_flags_tolerate_undef\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\app.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(main, argv, flags_parser)\u001B[0m\n\u001B[0;32m    295\u001B[0m     args = _run_init(\n\u001B[0;32m    296\u001B[0m         \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margv\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0margv\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 297\u001B[1;33m         \u001B[0mflags_parser\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    298\u001B[0m     )\n\u001B[0;32m    299\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[0m_init_callbacks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\app.py\u001B[0m in \u001B[0;36m_run_init\u001B[1;34m(argv, flags_parser)\u001B[0m\n\u001B[0;32m    361\u001B[0m   \u001B[0mcommand_name\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_process_name_useful\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m   \u001B[1;31m# Set up absl logging handler.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 363\u001B[1;33m   \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse_absl_handler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m   args = _register_and_parse_flags_with_usage(\n\u001B[0;32m    365\u001B[0m       \u001B[0margv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\logging\\__init__.py\u001B[0m in \u001B[0;36muse_absl_handler\u001B[1;34m()\u001B[0m\n\u001B[0;32m   1227\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mabsl_handler\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandlers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1228\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mabsl_handler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1229\u001B[1;33m     \u001B[0mFLAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'verbosity'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_logging_levels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1230\u001B[0m     \u001B[0mFLAGS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'logger_levels'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_logger_levels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1231\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    468\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    469\u001B[0m     \u001B[1;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 470\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flags\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    471\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    472\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_hide_flag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'verbosity'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # delete all flags, edit mm\n",
    "    def del_all_flags(FLAGS):\n",
    "        flags_dict = FLAGS._flags()\n",
    "        keys_list = [keys for keys in flags_dict]\n",
    "        for keys in keys_list:\n",
    "            FLAGS.__delattr__(keys)\n",
    "    del_all_flags(tf.compat.v1.flags.FLAGS)\n",
    "\n",
    "    flags = tf.compat.v1.app.flags # edited mm\n",
    "\n",
    "    # paths\n",
    "    flags.DEFINE_string('experiment_name', 'new_experiment', 'name of the experiment being run')\n",
    "    flags.DEFINE_string('ckpt_dir', os.path.join(ROOT_DIR, 'ckpts/'), 'checkpoint directory')\n",
    "    flags.DEFINE_string('log_dir', os.path.join(ROOT_DIR, 'logs/'), 'log directory')\n",
    "\n",
    "    # initialization\n",
    "    flags.DEFINE_string('init_ckpt_dir', None, 'path to checkpoint prefix from which to initialize weights (default None)')\n",
    "    flags.DEFINE_string('imagenet_weights_path', None, 'path to ImageNet weights for initialization (default None)')\n",
    "    flags.DEFINE_string('hs_weight_init', None, 'method for initializing weights of non-RGB bands in 1st conv layer, one of [None (default), \"random\", \"same\", \"samescaled\"]')\n",
    "    flags.DEFINE_boolean('exclude_final_layer', None, 'whether to use checkpoint to initialize final layer (default None)')\n",
    "\n",
    "    # learning parameters\n",
    "    flags.DEFINE_string('label_name', 'wealthpooled', 'name of label to use from the TFRecord files')\n",
    "    flags.DEFINE_integer('batch_size', 64, 'batch size')\n",
    "    flags.DEFINE_boolean('augment', True, 'whether to use data augmentation')\n",
    "    flags.DEFINE_float('fc_reg', 1e-3, 'Regularization penalty factor for fully connected layers')\n",
    "    flags.DEFINE_float('conv_reg', 1e-3, 'Regularization penalty factor for convolution layers')\n",
    "    flags.DEFINE_float('lr', 1e-3, 'Learning rate for optimizer')\n",
    "    flags.DEFINE_float('lr_decay', 1.0, 'Decay rate of the learning rate (default 1.0 for no decay)')\n",
    "\n",
    "    # high-level model control\n",
    "    flags.DEFINE_string('model_name', 'resnet', 'name of the model to be used, one of [\"resnet\" (default), \"vggf\", \"simplecnn\", \"resnetcombo\"]')\n",
    "\n",
    "    # resnet-only params\n",
    "    flags.DEFINE_integer('num_layers', 18, 'Number of ResNet layers, one of [18 (default), 34, 50]')\n",
    "\n",
    "    # data params\n",
    "    flags.DEFINE_string('batcher_type', 'base', 'batcher, one of [\"base\" (default), \"urban\", \"rural\"]')\n",
    "    flags.DEFINE_string('dataset', '2009-17', 'dataset to use, options depend on batcher_type (default \"2009-17\")')\n",
    "    flags.DEFINE_boolean('ooc', True, 'whether to use out-of-country split (default True)')\n",
    "    flags.DEFINE_float('keep_frac', 1.0, 'fraction of training data to use (default 1.0)')\n",
    "    flags.DEFINE_string('ls_bands', None, 'Landsat bands to use, one of [None (default), \"rgb\", \"ms\"]')\n",
    "    flags.DEFINE_string('nl_band', None, 'nightlights band, one of [None (default), \"merge\", \"split\"]')\n",
    "\n",
    "    # system\n",
    "    flags.DEFINE_integer('gpu', None, 'which GPU to use (default None)')\n",
    "    flags.DEFINE_integer('num_threads', 1, 'number of threads for batcher (default 1)')\n",
    "    flags.DEFINE_list('cache', [], 'comma-separated list (no spaces) of datasets to cache in memory, choose from [None, \"train\", \"train_eval\", \"val\"]')\n",
    "\n",
    "    # Misc\n",
    "    flags.DEFINE_integer('max_epochs', 150, 'maximum number of epochs for training (default 50)')\n",
    "    flags.DEFINE_integer('eval_every', 1, 'evaluate the model on the validation set after every so many epochs of training')\n",
    "    flags.DEFINE_integer('print_every', 10, 'print training statistics after every so many steps')\n",
    "    flags.DEFINE_integer('seed', 123, 'seed for random initialization and shuffling')\n",
    "\n",
    "    tf.compat.v1.app.run() # edited mm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}